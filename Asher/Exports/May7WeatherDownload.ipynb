{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting script\n"
     ]
    }
   ],
   "source": [
    "# dmi-station.py --- Get Daisy hourly weather data from a DMI station.\n",
    "\n",
    "\n",
    "# What data do we want\n",
    "PARS = [\"acc_precip\", \"mean_temp\", \"mean_relative_hum\", \"mean_wind_speed\",\n",
    "        \"mean_radiation\"]\n",
    "\n",
    "# Time resolution \n",
    "TIMERES = \"hour\"\n",
    "\n",
    "#Where we want data to go\n",
    "OUTPUT_FILE = \"dmidata_DTU.csv\"\n",
    "\n",
    "#Store information about data here\n",
    "META_FILE = \"dmimeta_DTU.csv\"\n",
    "\n",
    "# Your API key\n",
    "DMI_API_KEY = \"2e122380-2c21-46b8-8f4f-9d1c3ae0ecab\"\n",
    "\n",
    "print (\"Starting script\")\n",
    "\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import cos, asin, sqrt, pi\n",
    "import ast\n",
    "\n",
    "# open-dmi-data: client.py\n",
    "\n",
    "import requests\n",
    "from tenacity import retry, stop_after_attempt, wait_random\n",
    "\n",
    "class DMIOpenDataClient:\n",
    "    _base_url = \"https://dmigw.govcloud.dk/{version}/{api}\"\n",
    "\n",
    "    def __init__(self, api_key: str, api_name:str = \"metObs\", version: str = \"v2\"):\n",
    "        if api_key is None:\n",
    "            raise ValueError(f\"Invalid value for `api_key`: {api_key}\")\n",
    "        if api_name not in (\"climateData\", \"metObs\"):\n",
    "            raise NotImplementedError(f\"Following api is not supported yet: {api_name}\")\n",
    "        if version == \"v1\":\n",
    "            raise ValueError(f\"DMI metObs v1 not longer supported\")\n",
    "        if version not in [\"v2\"]:\n",
    "            raise ValueError(f\"API version {version} not supported\")\n",
    "\n",
    "        self.api_key = api_key\n",
    "        self.api_name = api_name\n",
    "        self.version = version\n",
    "\n",
    "    def base_url(self, api: str):\n",
    "        if api not in (\"climateData\", \"metObs\"):\n",
    "            raise NotImplementedError(f\"Following api is not supported yet: {api}\")\n",
    "        return self._base_url.format(version=self.version, api=api)\n",
    "\n",
    "    @retry(stop=stop_after_attempt(10), wait=wait_random(min=0.1, max=1.00))\n",
    "    def _query(self, api: str, service: str, params, **kwargs):\n",
    "        res = requests.get(\n",
    "            url=f\"{self.base_url(api=api)}/{service}\",\n",
    "            params={\n",
    "                \"api-key\": self.api_key,\n",
    "                **params,\n",
    "            },\n",
    "            **kwargs,\n",
    "        )\n",
    "        data = res.json()\n",
    "        http_status_code = data.get(\"http_status_code\", 200)\n",
    "        if http_status_code != 200:\n",
    "            message = data.get(\"message\")\n",
    "            raise ValueError(\n",
    "                f\"Failed HTTP request with HTTP status code {http_status_code} and message: {message}\"\n",
    "            )\n",
    "        return res.json()\n",
    "\n",
    "    def get_stations(\n",
    "            self, limit = 10000, offset = 0\n",
    "    ):\n",
    "        \"\"\"Get DMI stations.\n",
    "\n",
    "        Args:\n",
    "            limit (Optional[int], optional): Specify a maximum number of stations\n",
    "                you want to be returned. Defaults to 10000.\n",
    "            offset (Optional[int], optional): Specify the number of stations that should be skipped\n",
    "                before returning matching objects. Defaults to 0.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict[str, Any]]: List of DMI stations.\n",
    "        \"\"\"\n",
    "        res = self._query(\n",
    "            api=self.api_name,\n",
    "            service=\"collections/station/items\",\n",
    "            params={\n",
    "                \"limit\": limit,\n",
    "                \"offset\": offset,\n",
    "            },\n",
    "        )\n",
    "        return res.get(\"features\", [])\n",
    "\n",
    "    def get_observations(\n",
    "        self,\n",
    "        parameter = None,\n",
    "        station_id = None,\n",
    "        from_time = None,\n",
    "        to_time = None,\n",
    "        limit = 10000,\n",
    "        offset = 0,\n",
    "    ):\n",
    "        \"\"\"Get raw DMI observation.\n",
    "\n",
    "        Args:\n",
    "            parameter_id (Optional[Parameter], optional): Returns observations for a specific parameter.\n",
    "                Defaults to None.\n",
    "            station_id (Optional[int], optional): Search for a specific station using the stationID.\n",
    "                Defaults to None.\n",
    "            from_time (Optional[datetime], optional): Returns only objects with a \"timeObserved\" equal\n",
    "                to or after a given timestamp. Defaults to None.\n",
    "            to_time (Optional[datetime], optional): Returns only objects with a \"timeObserved\" before\n",
    "                (not including) a given timestamp. Defaults to None.\n",
    "            limit (Optional[int], optional): Specify a maximum number of observations\n",
    "                you want to be returned. Defaults to 10000.\n",
    "            offset (Optional[int], optional): Specify the number of observations that should be skipped\n",
    "                before returning matching objects. Defaults to 0.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict[str, Any]]: List of raw DMI observations.\n",
    "        \"\"\"\n",
    "        res = self._query(\n",
    "            api=\"metObs\",\n",
    "            service=\"collections/observation/items\",\n",
    "            params={\n",
    "                \"parameterId\": parameter,\n",
    "                \"stationId\": station_id,\n",
    "                \"datetime\": _construct_datetime_argument(\n",
    "                    from_time=from_time, to_time=to_time\n",
    "                ),\n",
    "                \"limit\": limit,\n",
    "                \"offset\": offset,\n",
    "            },\n",
    "        )\n",
    "        return res.get(\"features\", [])\n",
    "\n",
    "    def get_climate_data(\n",
    "        self,\n",
    "        parameter = None,\n",
    "        station_id = None,\n",
    "        from_time = None,\n",
    "        to_time = None,\n",
    "        time_resolution = None,\n",
    "        limit = 10000,\n",
    "        offset = 0,\n",
    "    ):\n",
    "        \"\"\"Get raw DMI climate data.\n",
    "\n",
    "        Args:\n",
    "            parameter_id (Optional[ClimateDataParameter], optional): Returns observations for a specific parameter.\n",
    "                Defaults to None.\n",
    "            station_id (Optional[int], optional): Search for a specific station using the stationID.\n",
    "                Defaults to None.\n",
    "            from_time (Optional[datetime], optional): Returns only objects with a \"timeObserved\" equal\n",
    "                to or after a given timestamp. Defaults to None.\n",
    "            to_time (Optional[datetime], optional): Returns only objects with a \"timeObserved\" before\n",
    "                (not including) a given timestamp. Defaults to None.\n",
    "            time_resolution (Optional[str], optional): Filter by time resolution (hour/day/month/year),\n",
    "                ie. what type of time interval the station value represents\n",
    "            limit (Optional[int], optional): Specify a maximum number of observations\n",
    "                you want to be returned. Defaults to 10000.\n",
    "            offset (Optional[int], optional): Specify the number of observations that should be skipped\n",
    "                before returning matching objects. Defaults to 0.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict[str, Any]]: List of raw DMI observations.\n",
    "        \"\"\"\n",
    "        res = self._query(\n",
    "            api=\"climateData\",\n",
    "            service=\"collections/stationValue/items\",\n",
    "            params={\n",
    "                \"parameterId\": parameter,\n",
    "                \"stationId\": station_id,\n",
    "                \"datetime\": _construct_datetime_argument(\n",
    "                    from_time=from_time, to_time=to_time\n",
    "                ),\n",
    "                \"timeResolution\": time_resolution,\n",
    "                \"limit\": limit,\n",
    "                \"offset\": offset,\n",
    "            },\n",
    "        )\n",
    "        return res.get(\"features\", [])\n",
    "\n",
    "    def get_closest_station(\n",
    "            self, latitude: float, longitude: float,\n",
    "            pars = []\n",
    "    ):\n",
    "        \"\"\"Get closest weather station from given coordinates.\n",
    "\n",
    "        Args:\n",
    "            latitude (float): Latitude coordinate.\n",
    "            longitude (float): Longitude coordinate.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict[str, Any]]: Closest weather station.\n",
    "        \"\"\"\n",
    "        stations = self.get_stations()\n",
    "        closest_station, closests_dist = None, 1e10\n",
    "        want_pars = set (pars)\n",
    "        for station in stations:\n",
    "            coordinates = station.get(\"geometry\", {}).get(\"coordinates\")\n",
    "            if coordinates is None or len(coordinates) < 2:\n",
    "                continue\n",
    "            lat, lon = coordinates[1], coordinates[0]\n",
    "            if lat is None or lon is None:\n",
    "                continue\n",
    "\n",
    "            has_pars = set (station['properties']['parameterId'])\n",
    "            if (not want_pars.issubset (has_pars)):\n",
    "                continue\n",
    "            \n",
    "            # Calculate distance\n",
    "            dist = distance(\n",
    "                lat1=latitude,\n",
    "                lon1=longitude,\n",
    "                lat2=lat,\n",
    "                lon2=lon,\n",
    "            )\n",
    "\n",
    "            if dist < closests_dist:\n",
    "                closests_dist, closest_station = dist, station\n",
    "        return closest_station\n",
    "\n",
    "\n",
    "def _construct_datetime_argument(\n",
    "    from_time = None, to_time = None\n",
    ") -> str:\n",
    "    if from_time is None and to_time is None:\n",
    "        return None\n",
    "    if from_time is not None and to_time is None:\n",
    "        return f\"{from_time.isoformat()}Z\"\n",
    "    if from_time is None and to_time is not None:\n",
    "        return f\"{to_time.isoformat()}Z\"\n",
    "    return f\"{from_time.isoformat()}Z/{to_time.isoformat()}Z\"\n",
    "\n",
    "# dmi-open-data: utils.py\n",
    "\n",
    "# Constants\n",
    "CONST_EARTH_RADIUS = 6371       # km\n",
    "CONST_EARTH_DIAMETER = 12742    # km\n",
    "EPOCH = datetime.utcfromtimestamp(0)\n",
    "\n",
    "# From Stackoverflow answer\n",
    "# https://stackoverflow.com/a/21623206/2538589\n",
    "def distance(lat1: float, lon1: float, lat2: float, lon2: float) -> float:\n",
    "    \"\"\"Calculate distance in km between two geographical points.\n",
    "\n",
    "    Args:\n",
    "        lat1 (float): Latitude of point 1.\n",
    "        lon1 (float): Longitude of point 1.\n",
    "        lat2 (float): Latitude of point 2.\n",
    "        lon2 (float): Longitude of point 2.\n",
    "\n",
    "    Returns:\n",
    "        float: Haversine distance in km between point 1 and 2.\n",
    "    \"\"\"\n",
    "    p = pi / 180.0\n",
    "    a = 0.5 - cos((lat2 - lat1) * p) / 2.0 + cos(lat1 * p) * cos(lat2 * p) * (1.0 - cos((lon2 - lon1) * p)) / 2\n",
    "    return CONST_EARTH_DIAMETER * asin(sqrt(a))  # 2*R*asin...\n",
    "\n",
    "# Get time series\n",
    "\n",
    "def getValue (data):\n",
    "    for i in data:\n",
    "        yield i['properties']['value']\n",
    "\n",
    "def getIndex (data):\n",
    "    for i in data:\n",
    "        yield np.datetime64 (i['properties']['to'])\n",
    "\n",
    "def getSeries (*, par, stationId, timeres):\n",
    "    print (\"Looking up parameter \", par)\n",
    "    data = client.get_climate_data(par,\n",
    "                                   station_id=stationId,\n",
    "                                   time_resolution=timeres,\n",
    "                                   limit=200000)\n",
    "    if (len (data) > 0):\n",
    "        print (\"Has \", len (data), \" datapoints\")\n",
    "    else:\n",
    "        print (\"No data, ignoring\")\n",
    "    return pd.Series (getValue (data),\n",
    "                      index=getIndex (data))\n",
    "\n",
    "def get_data (*, latitude, longitude, timeres, pars):\n",
    "    p = pd.DataFrame ()\n",
    "    m = pd.DataFrame (columns=[\"par\", \"id\", \"dist\", \"lat\", \"lon\"])\n",
    "    for par in pars:\n",
    "        print (\"Looking for station with \", par)\n",
    "        station = client.get_closest_station (latitude=latitude,\n",
    "                                              longitude=longitude,\n",
    "                                              pars=[par])\n",
    "        if (not station):\n",
    "            print (\"None found\");\n",
    "            continue\n",
    "        \n",
    "        coordinates = station.get(\"geometry\", {}).get(\"coordinates\")\n",
    "        station_lat, station_lon = coordinates[1], coordinates[0]\n",
    "        stationId = station['properties']['stationId']\n",
    "        dist = distance (lat1=latitude, lon1=longitude,\n",
    "                         lat2=station_lat, lon2=station_lon)\n",
    "        print (\"Found\", par, \"in station\", stationId, dist, \"km away\")\n",
    "        new_row = pd.DataFrame({'par': par, 'id': stationId, 'dist': dist,\n",
    "                                'lat': station_lat, 'lon': station_lon}, index=[0])\n",
    "        m = pd.concat ([new_row, m.loc[:]]).reset_index(drop=True)\n",
    "        s = getSeries (par=par, stationId=stationId, timeres=timeres)\n",
    "        if (len (s) > 0): \n",
    "            p[par] = s\n",
    "    return p, m\n",
    "\n",
    "\n",
    "    # dmi-station.py ends here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coords = pd.read_csv(\"/Users/asherkite/Desktop/School/Courses/MBML/MBML_Project/MBML_github/MBML-traffic-accidents/Asher/lat_lon.csv\")\n",
    "\n",
    "df_coords[\"cell_id\"] = df_coords[\"cell_id\"].apply(ast.literal_eval)\n",
    "df_coords[\"lat_lon\"] = df_coords[\"lat_lon\"].apply(ast.literal_eval)\n",
    "df_coords = df_coords.set_index(\"cell_id\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps:\n",
    "* when I get the p array, modify it so it has a multiindex of cell id and hour time (overarching location index over the entire dataframe? remember that the dataframe is at one location)\n",
    "* save the dataframe as a file\n",
    "* when I have all the files, stitch them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening DMI client\n",
      "Skipping 717514.445_6177963.565.csv (already exists)\n",
      "Skipping 717514.445_6178963.565.csv (already exists)\n",
      "Skipping 717514.445_6179463.565.csv (already exists)\n",
      "Skipping 717514.445_6179963.565.csv (already exists)\n",
      "Skipping 718014.445_6176963.565.csv (already exists)\n",
      "Skipping 718014.445_6177963.565.csv (already exists)\n",
      "Skipping 718014.445_6178963.565.csv (already exists)\n",
      "Skipping 718014.445_6179463.565.csv (already exists)\n",
      "Skipping 718514.445_6174963.565.csv (already exists)\n",
      "Skipping 718514.445_6176963.565.csv (already exists)\n",
      "Skipping 718514.445_6177463.565.csv (already exists)\n",
      "Skipping 718514.445_6177963.565.csv (already exists)\n",
      "Skipping 718514.445_6178463.565.csv (already exists)\n",
      "Skipping 718514.445_6178963.565.csv (already exists)\n",
      "Skipping 718514.445_6179463.565.csv (already exists)\n",
      "Skipping 718514.445_6180463.565.csv (already exists)\n",
      "Skipping 719014.445_6173463.565.csv (already exists)\n",
      "Skipping 719014.445_6173963.565.csv (already exists)\n",
      "Skipping 719014.445_6174463.565.csv (already exists)\n",
      "Skipping 719014.445_6174963.565.csv (already exists)\n",
      "Skipping 719014.445_6175463.565.csv (already exists)\n",
      "Skipping 719014.445_6175963.565.csv (already exists)\n",
      "Skipping 719014.445_6176463.565.csv (already exists)\n",
      "Skipping 719014.445_6176963.565.csv (already exists)\n",
      "Skipping 719014.445_6177463.565.csv (already exists)\n",
      "Skipping 719014.445_6177963.565.csv (already exists)\n",
      "Skipping 719014.445_6178963.565.csv (already exists)\n",
      "Skipping 719014.445_6179463.565.csv (already exists)\n",
      "Skipping 719014.445_6179963.565.csv (already exists)\n",
      "Skipping 719014.445_6180963.565.csv (already exists)\n",
      "Skipping 719514.445_6172463.565.csv (already exists)\n",
      "Skipping 719514.445_6172963.565.csv (already exists)\n",
      "Skipping 719514.445_6173463.565.csv (already exists)\n",
      "Skipping 719514.445_6173963.565.csv (already exists)\n",
      "Skipping 719514.445_6174463.565.csv (already exists)\n",
      "Skipping 719514.445_6174963.565.csv (already exists)\n",
      "Skipping 719514.445_6175463.565.csv (already exists)\n",
      "Skipping 719514.445_6175963.565.csv (already exists)\n",
      "Skipping 719514.445_6176463.565.csv (already exists)\n",
      "Skipping 719514.445_6176963.565.csv (already exists)\n",
      "Skipping 719514.445_6177463.565.csv (already exists)\n",
      "Skipping 719514.445_6178963.565.csv (already exists)\n",
      "Skipping 719514.445_6179963.565.csv (already exists)\n",
      "Skipping 719514.445_6180463.565.csv (already exists)\n",
      "Skipping 720014.445_6172463.565.csv (already exists)\n",
      "Skipping 720014.445_6172963.565.csv (already exists)\n",
      "Skipping 720014.445_6173463.565.csv (already exists)\n",
      "Skipping 720014.445_6173963.565.csv (already exists)\n",
      "Skipping 720014.445_6174463.565.csv (already exists)\n",
      "Skipping 720014.445_6174963.565.csv (already exists)\n",
      "Skipping 720014.445_6176963.565.csv (already exists)\n",
      "Skipping 720014.445_6177463.565.csv (already exists)\n",
      "Skipping 720014.445_6177963.565.csv (already exists)\n",
      "Skipping 720014.445_6178963.565.csv (already exists)\n",
      "Skipping 720014.445_6179463.565.csv (already exists)\n",
      "Skipping 720514.445_6171963.565.csv (already exists)\n",
      "Skipping 720514.445_6172463.565.csv (already exists)\n",
      "Skipping 720514.445_6172963.565.csv (already exists)\n",
      "Skipping 720514.445_6173463.565.csv (already exists)\n",
      "Skipping 720514.445_6173963.565.csv (already exists)\n",
      "Skipping 720514.445_6174463.565.csv (already exists)\n",
      "Skipping 720514.445_6174963.565.csv (already exists)\n",
      "Skipping 720514.445_6176963.565.csv (already exists)\n",
      "Skipping 720514.445_6177463.565.csv (already exists)\n",
      "Skipping 720514.445_6177963.565.csv (already exists)\n",
      "Skipping 720514.445_6178463.565.csv (already exists)\n",
      "Skipping 720514.445_6178963.565.csv (already exists)\n",
      "Skipping 720514.445_6179463.565.csv (already exists)\n",
      "Skipping 720514.445_6179963.565.csv (already exists)\n",
      "Skipping 721014.445_6168963.565.csv (already exists)\n",
      "Skipping 721014.445_6172463.565.csv (already exists)\n",
      "Skipping 721014.445_6172963.565.csv (already exists)\n",
      "Skipping 721014.445_6173463.565.csv (already exists)\n",
      "Skipping 721014.445_6173963.565.csv (already exists)\n",
      "Skipping 721014.445_6174463.565.csv (already exists)\n",
      "Skipping 721014.445_6177463.565.csv (already exists)\n",
      "Skipping 721014.445_6177963.565.csv (already exists)\n",
      "Skipping 721014.445_6178463.565.csv (already exists)\n",
      "Skipping 721014.445_6178963.565.csv (already exists)\n",
      "Skipping 721014.445_6179463.565.csv (already exists)\n",
      "Skipping 721014.445_6179963.565.csv (already exists)\n",
      "Skipping 721014.445_6180463.565.csv (already exists)\n",
      "Skipping 721514.445_6168963.565.csv (already exists)\n",
      "Skipping 721514.445_6172463.565.csv (already exists)\n",
      "Skipping 721514.445_6172963.565.csv (already exists)\n",
      "Skipping 721514.445_6173963.565.csv (already exists)\n",
      "Skipping 721514.445_6174463.565.csv (already exists)\n",
      "Skipping 721514.445_6177963.565.csv (already exists)\n",
      "Skipping 721514.445_6178463.565.csv (already exists)\n",
      "Skipping 721514.445_6178963.565.csv (already exists)\n",
      "Skipping 721514.445_6179463.565.csv (already exists)\n",
      "Skipping 721514.445_6180463.565.csv (already exists)\n",
      "Skipping 721514.445_6180963.565.csv (already exists)\n",
      "Skipping 721514.445_6181463.565.csv (already exists)\n",
      "Skipping 722014.445_6168963.565.csv (already exists)\n",
      "Skipping 722014.445_6169463.565.csv (already exists)\n",
      "Skipping 722014.445_6171963.565.csv (already exists)\n",
      "Skipping 722014.445_6172463.565.csv (already exists)\n",
      "Skipping 722014.445_6172963.565.csv (already exists)\n",
      "Skipping 722014.445_6173463.565.csv (already exists)\n",
      "Skipping 722014.445_6173963.565.csv (already exists)\n",
      "Skipping 722014.445_6174463.565.csv (already exists)\n",
      "Skipping 722014.445_6174963.565.csv (already exists)\n",
      "Skipping 722014.445_6177963.565.csv (already exists)\n",
      "Skipping 722014.445_6178463.565.csv (already exists)\n",
      "Skipping 722014.445_6178963.565.csv (already exists)\n",
      "Skipping 722014.445_6179463.565.csv (already exists)\n",
      "Skipping 722014.445_6179963.565.csv (already exists)\n",
      "Skipping 722014.445_6180463.565.csv (already exists)\n",
      "Skipping 722014.445_6180963.565.csv (already exists)\n",
      "Skipping 722514.445_6169463.565.csv (already exists)\n",
      "Skipping 722514.445_6169963.565.csv (already exists)\n",
      "Skipping 722514.445_6170463.565.csv (already exists)\n",
      "Skipping 722514.445_6171963.565.csv (already exists)\n",
      "Skipping 722514.445_6172463.565.csv (already exists)\n",
      "Skipping 722514.445_6172963.565.csv (already exists)\n",
      "Skipping 722514.445_6173463.565.csv (already exists)\n",
      "Skipping 722514.445_6173963.565.csv (already exists)\n",
      "Skipping 722514.445_6174463.565.csv (already exists)\n",
      "Skipping 722514.445_6174963.565.csv (already exists)\n",
      "Skipping 722514.445_6176963.565.csv (already exists)\n",
      "Skipping 722514.445_6177463.565.csv (already exists)\n",
      "Skipping 722514.445_6177963.565.csv (already exists)\n",
      "Skipping 722514.445_6178463.565.csv (already exists)\n",
      "Skipping 722514.445_6178963.565.csv (already exists)\n",
      "Skipping 722514.445_6179463.565.csv (already exists)\n",
      "Skipping 722514.445_6179963.565.csv (already exists)\n",
      "Skipping 722514.445_6180463.565.csv (already exists)\n",
      "Skipping 722514.445_6180963.565.csv (already exists)\n",
      "Skipping 723014.445_6170463.565.csv (already exists)\n",
      "Skipping 723014.445_6170963.565.csv (already exists)\n",
      "Skipping 723014.445_6171963.565.csv (already exists)\n",
      "Skipping 723014.445_6172463.565.csv (already exists)\n",
      "Skipping 723014.445_6172963.565.csv (already exists)\n",
      "Skipping 723014.445_6173463.565.csv (already exists)\n",
      "Skipping 723014.445_6174463.565.csv (already exists)\n",
      "Skipping 723014.445_6174963.565.csv (already exists)\n",
      "Skipping 723014.445_6175463.565.csv (already exists)\n",
      "Skipping 723014.445_6176963.565.csv (already exists)\n",
      "Skipping 723014.445_6177463.565.csv (already exists)\n",
      "Skipping 723014.445_6177963.565.csv (already exists)\n",
      "Skipping 723014.445_6178463.565.csv (already exists)\n",
      "Skipping 723014.445_6178963.565.csv (already exists)\n",
      "Skipping 723014.445_6179463.565.csv (already exists)\n",
      "Skipping 723014.445_6180463.565.csv (already exists)\n",
      "Skipping 723014.445_6180963.565.csv (already exists)\n",
      "Skipping 723514.445_6170463.565.csv (already exists)\n",
      "Skipping 723514.445_6170963.565.csv (already exists)\n",
      "Skipping 723514.445_6171463.565.csv (already exists)\n",
      "Skipping 723514.445_6171963.565.csv (already exists)\n",
      "Skipping 723514.445_6172463.565.csv (already exists)\n",
      "Skipping 723514.445_6172963.565.csv (already exists)\n",
      "Skipping 723514.445_6173963.565.csv (already exists)\n",
      "Skipping 723514.445_6174463.565.csv (already exists)\n",
      "Skipping 723514.445_6174963.565.csv (already exists)\n",
      "Skipping 723514.445_6175463.565.csv (already exists)\n",
      "Skipping 723514.445_6175963.565.csv (already exists)\n",
      "Skipping 723514.445_6176463.565.csv (already exists)\n",
      "Skipping 723514.445_6176963.565.csv (already exists)\n",
      "Skipping 723514.445_6177463.565.csv (already exists)\n",
      "Skipping 723514.445_6177963.565.csv (already exists)\n",
      "Skipping 723514.445_6178463.565.csv (already exists)\n",
      "Skipping 723514.445_6178963.565.csv (already exists)\n",
      "Skipping 723514.445_6179463.565.csv (already exists)\n",
      "Skipping 723514.445_6179963.565.csv (already exists)\n",
      "Skipping 723514.445_6180463.565.csv (already exists)\n",
      "Skipping 723514.445_6180963.565.csv (already exists)\n",
      "Skipping 723514.445_6181463.565.csv (already exists)\n",
      "Skipping 724014.445_6170463.565.csv (already exists)\n",
      "Skipping 724014.445_6171963.565.csv (already exists)\n",
      "Skipping 724014.445_6173463.565.csv (already exists)\n",
      "Skipping 724014.445_6173963.565.csv (already exists)\n",
      "Skipping 724014.445_6174463.565.csv (already exists)\n",
      "Skipping 724014.445_6174963.565.csv (already exists)\n",
      "Skipping 724014.445_6175463.565.csv (already exists)\n",
      "Skipping 724014.445_6175963.565.csv (already exists)\n",
      "Skipping 724014.445_6176463.565.csv (already exists)\n",
      "Skipping 724014.445_6176963.565.csv (already exists)\n",
      "Skipping 724014.445_6177463.565.csv (already exists)\n",
      "Skipping 724014.445_6177963.565.csv (already exists)\n",
      "Skipping 724014.445_6178463.565.csv (already exists)\n",
      "Skipping 724014.445_6178963.565.csv (already exists)\n",
      "Skipping 724014.445_6179463.565.csv (already exists)\n",
      "Skipping 724014.445_6179963.565.csv (already exists)\n",
      "Skipping 724014.445_6180463.565.csv (already exists)\n",
      "Skipping 724014.445_6181463.565.csv (already exists)\n",
      "Skipping 724514.445_6169963.565.csv (already exists)\n",
      "Skipping 724514.445_6171963.565.csv (already exists)\n",
      "Skipping 724514.445_6172463.565.csv (already exists)\n",
      "Skipping 724514.445_6172963.565.csv (already exists)\n",
      "Skipping 724514.445_6173463.565.csv (already exists)\n",
      "Skipping 724514.445_6173963.565.csv (already exists)\n",
      "Skipping 724514.445_6174463.565.csv (already exists)\n",
      "Skipping 724514.445_6174963.565.csv (already exists)\n",
      "Skipping 724514.445_6175463.565.csv (already exists)\n",
      "Skipping 724514.445_6175963.565.csv (already exists)\n",
      "Skipping 724514.445_6176463.565.csv (already exists)\n",
      "Skipping 724514.445_6176963.565.csv (already exists)\n",
      "Skipping 724514.445_6177463.565.csv (already exists)\n",
      "Skipping 724514.445_6177963.565.csv (already exists)\n",
      "Skipping 724514.445_6178463.565.csv (already exists)\n",
      "Skipping 724514.445_6178963.565.csv (already exists)\n",
      "Skipping 724514.445_6179463.565.csv (already exists)\n",
      "Skipping 724514.445_6179963.565.csv (already exists)\n",
      "Skipping 724514.445_6180463.565.csv (already exists)\n",
      "Skipping 724514.445_6180963.565.csv (already exists)\n",
      "Skipping 724514.445_6181463.565.csv (already exists)\n",
      "Skipping 725014.445_6169463.565.csv (already exists)\n",
      "Skipping 725014.445_6169963.565.csv (already exists)\n",
      "Skipping 725014.445_6170463.565.csv (already exists)\n",
      "Skipping 725014.445_6170963.565.csv (already exists)\n",
      "Skipping 725014.445_6171463.565.csv (already exists)\n",
      "Skipping 725014.445_6171963.565.csv (already exists)\n",
      "Skipping 725014.445_6173963.565.csv (already exists)\n",
      "Skipping 725014.445_6174463.565.csv (already exists)\n",
      "Skipping 725014.445_6174963.565.csv (already exists)\n",
      "Skipping 725014.445_6175463.565.csv (already exists)\n",
      "Skipping 725014.445_6175963.565.csv (already exists)\n",
      "Skipping 725014.445_6176463.565.csv (already exists)\n",
      "Skipping 725014.445_6176963.565.csv (already exists)\n",
      "Skipping 725014.445_6177463.565.csv (already exists)\n",
      "Skipping 725014.445_6177963.565.csv (already exists)\n",
      "Skipping 725014.445_6178463.565.csv (already exists)\n",
      "Skipping 725014.445_6178963.565.csv (already exists)\n",
      "Skipping 725014.445_6179463.565.csv (already exists)\n",
      "Skipping 725014.445_6179963.565.csv (already exists)\n",
      "Skipping 725014.445_6180463.565.csv (already exists)\n",
      "Skipping 725014.445_6180963.565.csv (already exists)\n",
      "Skipping 725514.445_6170463.565.csv (already exists)\n",
      "Skipping 725514.445_6170963.565.csv (already exists)\n",
      "Skipping 725514.445_6171463.565.csv (already exists)\n",
      "Skipping 725514.445_6171963.565.csv (already exists)\n",
      "Skipping 725514.445_6172463.565.csv (already exists)\n",
      "Skipping 725514.445_6173463.565.csv (already exists)\n",
      "Skipping 725514.445_6173963.565.csv (already exists)\n",
      "Skipping 725514.445_6174463.565.csv (already exists)\n",
      "Skipping 725514.445_6174963.565.csv (already exists)\n",
      "Skipping 725514.445_6175463.565.csv (already exists)\n",
      "Skipping 725514.445_6175963.565.csv (already exists)\n",
      "Skipping 725514.445_6176463.565.csv (already exists)\n",
      "Skipping 725514.445_6176963.565.csv (already exists)\n",
      "Skipping 725514.445_6177463.565.csv (already exists)\n",
      "Skipping 725514.445_6177963.565.csv (already exists)\n",
      "Skipping 725514.445_6178463.565.csv (already exists)\n",
      "Skipping 725514.445_6178963.565.csv (already exists)\n",
      "Skipping 725514.445_6179463.565.csv (already exists)\n",
      "Skipping 725514.445_6179963.565.csv (already exists)\n",
      "Skipping 726014.445_6169963.565.csv (already exists)\n",
      "Skipping 726014.445_6170463.565.csv (already exists)\n",
      "Skipping 726014.445_6170963.565.csv (already exists)\n",
      "Skipping 726014.445_6171963.565.csv (already exists)\n",
      "Skipping 726014.445_6172463.565.csv (already exists)\n",
      "Skipping 726014.445_6172963.565.csv (already exists)\n",
      "Skipping 726014.445_6173463.565.csv (already exists)\n",
      "Skipping 726014.445_6173963.565.csv (already exists)\n",
      "Skipping 726014.445_6174463.565.csv (already exists)\n",
      "Skipping 726014.445_6174963.565.csv (already exists)\n",
      "Skipping 726014.445_6175463.565.csv (already exists)\n",
      "Skipping 726014.445_6175963.565.csv (already exists)\n",
      "Skipping 726014.445_6176463.565.csv (already exists)\n",
      "Skipping 726014.445_6176963.565.csv (already exists)\n",
      "Skipping 726014.445_6177463.565.csv (already exists)\n",
      "Skipping 726014.445_6177963.565.csv (already exists)\n",
      "Skipping 726014.445_6178463.565.csv (already exists)\n",
      "Skipping 726014.445_6178963.565.csv (already exists)\n",
      "Skipping 726014.445_6179963.565.csv (already exists)\n",
      "Skipping 726014.445_6180463.565.csv (already exists)\n",
      "Skipping 726514.445_6171963.565.csv (already exists)\n",
      "Skipping 726514.445_6172463.565.csv (already exists)\n",
      "Skipping 726514.445_6173463.565.csv (already exists)\n",
      "Skipping 726514.445_6173963.565.csv (already exists)\n",
      "Skipping 726514.445_6174463.565.csv (already exists)\n",
      "Skipping 726514.445_6174963.565.csv (already exists)\n",
      "Skipping 726514.445_6175963.565.csv (already exists)\n",
      "Skipping 726514.445_6176463.565.csv (already exists)\n",
      "Skipping 727014.445_6171963.565.csv (already exists)\n",
      "Skipping 727014.445_6172463.565.csv (already exists)\n",
      "Skipping 727014.445_6172963.565.csv (already exists)\n",
      "Skipping 727014.445_6173463.565.csv (already exists)\n",
      "Skipping 727014.445_6173963.565.csv (already exists)\n",
      "Skipping 727014.445_6174963.565.csv (already exists)\n",
      "Skipping 727014.445_6175463.565.csv (already exists)\n",
      "Skipping 727014.445_6175963.565.csv (already exists)\n",
      "Skipping 727014.445_6176963.565.csv (already exists)\n",
      "Skipping 727014.445_6177963.565.csv (already exists)\n",
      "Skipping 727514.445_6171963.565.csv (already exists)\n",
      "Skipping 727514.445_6172463.565.csv (already exists)\n",
      "Skipping 727514.445_6172963.565.csv (already exists)\n",
      "Skipping 727514.445_6173463.565.csv (already exists)\n",
      "Skipping 727514.445_6173963.565.csv (already exists)\n",
      "Skipping 727514.445_6174463.565.csv (already exists)\n",
      "Skipping 727514.445_6174963.565.csv (already exists)\n",
      "Skipping 727514.445_6175963.565.csv (already exists)\n",
      "Skipping 727514.445_6179963.565.csv (already exists)\n",
      "Skipping 728014.445_6172463.565.csv (already exists)\n",
      "Skipping 728014.445_6172963.565.csv (already exists)\n",
      "Skipping 728014.445_6173463.565.csv (already exists)\n",
      "Skipping 728014.445_6173963.565.csv (already exists)\n",
      "Skipping 728014.445_6174463.565.csv (already exists)\n",
      "Skipping 728014.445_6174963.565.csv (already exists)\n",
      "Skipping 728514.445_6172463.565.csv (already exists)\n",
      "Skipping 728514.445_6172963.565.csv (already exists)\n",
      "Skipping 728514.445_6173963.565.csv (already exists)\n",
      "Skipping 728514.445_6174463.565.csv (already exists)\n",
      "Skipping 729014.445_6172463.565.csv (already exists)\n",
      "Skipping 729014.445_6172963.565.csv (already exists)\n",
      "Skipping 729014.445_6173463.565.csv (already exists)\n",
      "Skipping 729014.445_6173963.565.csv (already exists)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'  OUTPUT_FILE = str(LATITUDE)+\"_\"+str(LONGITUDE)+\"_dmidata_DTU.csv\"\\n    META_FILE   = str(LATITUDE)+\"_\"+str(LONGITUDE)+\"_dmimeta_DTU.csv\"\\n\\n    [p, m] = get_data (latitude=LATITUDE, longitude=LONGITUDE, timeres=TIMERES,\\n                   pars=PARS)\\n\\n    Export_Path = \"/Users/asherkite/Desktop/School/Courses/MBML/MBML_Project/MBML_github/MBML-traffic-accidents/Asher/Exports\"\\n    # Sorting\\n    p.sort_index (inplace=True)\\n\\n    # Write it.\\n    p.to_csv (OUTPUT_FILE)\\n    m.to_csv (META_FILE) '"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"Opening DMI client\")\n",
    "#client = DMIOpenDataClient(api_key=DMI_API_KEY, api_name=\"climateData\")\n",
    "import os\n",
    "from pyproj import Transformer\n",
    "reverse_transformer = Transformer.from_crs(\"EPSG:4326\",\"EPSG:25832\", always_xy=True) #from normal lat/lon to x/y, NOTE: accepts (lon,lat)\n",
    "\n",
    "itnum = 0\n",
    "stopnum = len(df_coords)+1\n",
    "for index, row in df_coords.iterrows():\n",
    "    if itnum < stopnum:\n",
    "        itnum +=1\n",
    "        outdex= str(index[0]) + \"_\" + str(index[1])\n",
    "        OUTPUT_FILE = outdex+\".csv\"\n",
    "        META_FILE = outdex +\"_meta.csv\"\n",
    "        EXPORT = \"/Users/asherkite/Desktop/School/Courses/MBML/MBML_Project/MBML_github/MBML-traffic-accidents/Asher/Exports/\"\n",
    "        WEATHER = \"Weather/\"\n",
    "        METADATA = \"Metadata/\"\n",
    "        if os.path.exists(EXPORT+WEATHER+OUTPUT_FILE):\n",
    "            print(f\"Skipping {OUTPUT_FILE} (already exists)\")\n",
    "            continue\n",
    "        \n",
    "        LATITUDE, LONGITUDE = row[\"lat_lon\"]  # unpack the tuple\n",
    "        print(f\"Latitude: {LATITUDE}, Longitude: {LONGITUDE}\")\n",
    "        [p, m] = get_data (latitude=LATITUDE, longitude=LONGITUDE, timeres=TIMERES,\n",
    "                   pars=PARS)\n",
    "        p.sort_index (inplace=True)\n",
    "        p.index = pd.to_datetime(p.index)\n",
    "        p = p[p.index.year == 2023]\n",
    "        p[\"cell_id\"] = [index] * len(p)\n",
    "        p = p.reset_index().set_index([\"cell_id\", \"index\"]).sort_index()\n",
    "        p.index.names = [\"cell_id\", \"datetime\"]\n",
    "        m[\"cell_id\"] = [index] * len(m)\n",
    "        m = m.reset_index().set_index([\"cell_id\"]).sort_index()\n",
    "        m.index.names = [\"cell_id\"]\n",
    "        m = m.drop(columns=\"index\")\n",
    "        m[\"station_x_y\"] = m.apply(lambda row: reverse_transformer.transform(row[\"lon\"], row[\"lat\"]), axis=1)\n",
    "        p.to_csv (EXPORT+WEATHER+OUTPUT_FILE)\n",
    "        m.to_csv (EXPORT+METADATA+META_FILE)\n",
    "\n",
    "\"\"\"  OUTPUT_FILE = str(LATITUDE)+\"_\"+str(LONGITUDE)+\"_dmidata_DTU.csv\"\n",
    "    META_FILE   = str(LATITUDE)+\"_\"+str(LONGITUDE)+\"_dmimeta_DTU.csv\"\n",
    "\n",
    "    [p, m] = get_data (latitude=LATITUDE, longitude=LONGITUDE, timeres=TIMERES,\n",
    "                   pars=PARS)\n",
    "\n",
    "    Export_Path = \"/Users/asherkite/Desktop/School/Courses/MBML/MBML_Project/MBML_github/MBML-traffic-accidents/Asher/Exports\"\n",
    "    # Sorting\n",
    "    p.sort_index (inplace=True)\n",
    "\n",
    "    # Write it.\n",
    "    p.to_csv (OUTPUT_FILE)\n",
    "    m.to_csv (META_FILE) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Iteration: 2\n",
      "Iteration: 3\n",
      "Iteration: 4\n",
      "Iteration: 5\n",
      "Iteration: 6\n",
      "Iteration: 7\n",
      "Iteration: 8\n",
      "Iteration: 9\n",
      "Iteration: 10\n",
      "Iteration: 11\n",
      "Iteration: 12\n",
      "Iteration: 13\n",
      "Iteration: 14\n",
      "Iteration: 15\n",
      "Iteration: 16\n",
      "Iteration: 17\n",
      "Iteration: 18\n",
      "Iteration: 19\n",
      "Iteration: 20\n",
      "Iteration: 21\n",
      "Iteration: 22\n",
      "Iteration: 23\n",
      "Iteration: 24\n",
      "Iteration: 25\n",
      "Iteration: 26\n",
      "Iteration: 27\n",
      "Iteration: 28\n",
      "Iteration: 29\n",
      "Iteration: 30\n",
      "Iteration: 31\n",
      "Iteration: 32\n",
      "Iteration: 33\n",
      "Iteration: 34\n",
      "Iteration: 35\n",
      "Iteration: 36\n",
      "Iteration: 37\n",
      "Iteration: 38\n",
      "Iteration: 39\n",
      "Iteration: 40\n",
      "Iteration: 41\n",
      "Iteration: 42\n",
      "Iteration: 43\n",
      "Iteration: 44\n",
      "Iteration: 45\n",
      "Iteration: 46\n",
      "Iteration: 47\n",
      "Iteration: 48\n",
      "Iteration: 49\n",
      "Iteration: 50\n",
      "Iteration: 51\n",
      "Iteration: 52\n",
      "Iteration: 53\n",
      "Iteration: 54\n",
      "Iteration: 55\n",
      "Iteration: 56\n",
      "Iteration: 57\n",
      "Iteration: 58\n",
      "Iteration: 59\n",
      "Iteration: 60\n",
      "Iteration: 61\n",
      "Iteration: 62\n",
      "Iteration: 63\n",
      "Iteration: 64\n",
      "Iteration: 65\n",
      "Iteration: 66\n",
      "Iteration: 67\n",
      "Iteration: 68\n",
      "Iteration: 69\n",
      "Iteration: 70\n",
      "Iteration: 71\n",
      "Iteration: 72\n",
      "Iteration: 73\n",
      "Iteration: 74\n",
      "Iteration: 75\n",
      "Iteration: 76\n",
      "Iteration: 77\n",
      "Iteration: 78\n",
      "Iteration: 79\n",
      "Iteration: 80\n",
      "Iteration: 81\n",
      "Iteration: 82\n",
      "Iteration: 83\n",
      "Iteration: 84\n",
      "Iteration: 85\n",
      "Iteration: 86\n",
      "Iteration: 87\n",
      "Iteration: 88\n",
      "Iteration: 89\n",
      "Iteration: 90\n",
      "Iteration: 91\n",
      "Iteration: 92\n",
      "Iteration: 93\n",
      "Iteration: 94\n",
      "Iteration: 95\n",
      "Iteration: 96\n",
      "Iteration: 97\n",
      "Iteration: 98\n",
      "Iteration: 99\n",
      "Iteration: 100\n",
      "Iteration: 101\n",
      "Iteration: 102\n",
      "Iteration: 103\n",
      "Iteration: 104\n",
      "Iteration: 105\n",
      "Iteration: 106\n",
      "Iteration: 107\n",
      "Iteration: 108\n",
      "Iteration: 109\n",
      "Iteration: 110\n",
      "Iteration: 111\n",
      "Iteration: 112\n",
      "Iteration: 113\n",
      "Iteration: 114\n",
      "Iteration: 115\n",
      "Iteration: 116\n",
      "Iteration: 117\n",
      "Iteration: 118\n",
      "Iteration: 119\n",
      "Iteration: 120\n",
      "Iteration: 121\n",
      "Iteration: 122\n",
      "Iteration: 123\n",
      "Iteration: 124\n",
      "Iteration: 125\n",
      "Iteration: 126\n",
      "Iteration: 127\n",
      "Iteration: 128\n",
      "Iteration: 129\n",
      "Iteration: 130\n",
      "Iteration: 131\n",
      "Iteration: 132\n",
      "Iteration: 133\n",
      "Iteration: 134\n",
      "Iteration: 135\n",
      "Iteration: 136\n",
      "Iteration: 137\n",
      "Iteration: 138\n",
      "Iteration: 139\n",
      "Iteration: 140\n",
      "Iteration: 141\n",
      "Iteration: 142\n",
      "Iteration: 143\n",
      "Iteration: 144\n",
      "Iteration: 145\n",
      "Iteration: 146\n",
      "Iteration: 147\n",
      "Iteration: 148\n",
      "Iteration: 149\n",
      "Iteration: 150\n",
      "Iteration: 151\n",
      "Iteration: 152\n",
      "Iteration: 153\n",
      "Iteration: 154\n",
      "Iteration: 155\n",
      "Iteration: 156\n",
      "Iteration: 157\n",
      "Iteration: 158\n",
      "Iteration: 159\n",
      "Iteration: 160\n",
      "Iteration: 161\n",
      "Iteration: 162\n",
      "Iteration: 163\n",
      "Iteration: 164\n",
      "Iteration: 165\n",
      "Iteration: 166\n",
      "Iteration: 167\n",
      "Iteration: 168\n",
      "Iteration: 169\n",
      "Iteration: 170\n",
      "Iteration: 171\n",
      "Iteration: 172\n",
      "Iteration: 173\n",
      "Iteration: 174\n",
      "Iteration: 175\n",
      "Iteration: 176\n",
      "Iteration: 177\n",
      "Iteration: 178\n",
      "Iteration: 179\n",
      "Iteration: 180\n",
      "Iteration: 181\n",
      "Iteration: 182\n",
      "Iteration: 183\n",
      "Iteration: 184\n",
      "Iteration: 185\n",
      "Iteration: 186\n",
      "Iteration: 187\n",
      "Iteration: 188\n",
      "Iteration: 189\n",
      "Iteration: 190\n",
      "Iteration: 191\n",
      "Iteration: 192\n",
      "Iteration: 193\n",
      "Iteration: 194\n",
      "Iteration: 195\n",
      "Iteration: 196\n",
      "Iteration: 197\n",
      "Iteration: 198\n",
      "Iteration: 199\n",
      "Iteration: 200\n",
      "Iteration: 201\n",
      "Iteration: 202\n",
      "Iteration: 203\n",
      "Iteration: 204\n",
      "Iteration: 205\n",
      "Iteration: 206\n",
      "Iteration: 207\n",
      "Iteration: 208\n",
      "Iteration: 209\n",
      "Iteration: 210\n",
      "Iteration: 211\n",
      "Iteration: 212\n",
      "Iteration: 213\n",
      "Iteration: 214\n",
      "Iteration: 215\n",
      "Iteration: 216\n",
      "Iteration: 217\n",
      "Iteration: 218\n",
      "Iteration: 219\n",
      "Iteration: 220\n",
      "Iteration: 221\n",
      "Iteration: 222\n",
      "Iteration: 223\n",
      "Iteration: 224\n",
      "Iteration: 225\n",
      "Iteration: 226\n",
      "Iteration: 227\n",
      "Iteration: 228\n",
      "Iteration: 229\n",
      "Iteration: 230\n",
      "Iteration: 231\n",
      "Iteration: 232\n",
      "Iteration: 233\n",
      "Iteration: 234\n",
      "Iteration: 235\n",
      "Iteration: 236\n",
      "Iteration: 237\n",
      "Iteration: 238\n",
      "Iteration: 239\n",
      "Iteration: 240\n",
      "Iteration: 241\n",
      "Iteration: 242\n",
      "Iteration: 243\n",
      "Iteration: 244\n",
      "Iteration: 245\n",
      "Iteration: 246\n",
      "Iteration: 247\n",
      "Iteration: 248\n",
      "Iteration: 249\n",
      "Iteration: 250\n",
      "Iteration: 251\n",
      "Iteration: 252\n",
      "Iteration: 253\n",
      "Iteration: 254\n",
      "Iteration: 255\n",
      "Iteration: 256\n",
      "Iteration: 257\n",
      "Iteration: 258\n",
      "Iteration: 259\n",
      "Iteration: 260\n",
      "Iteration: 261\n",
      "Iteration: 262\n",
      "Iteration: 263\n",
      "Iteration: 264\n",
      "Iteration: 265\n",
      "Iteration: 266\n",
      "Iteration: 267\n",
      "Iteration: 268\n",
      "Iteration: 269\n",
      "Iteration: 270\n",
      "Iteration: 271\n",
      "Iteration: 272\n",
      "Iteration: 273\n",
      "Iteration: 274\n",
      "Iteration: 275\n",
      "Iteration: 276\n",
      "Iteration: 277\n",
      "Iteration: 278\n",
      "Iteration: 279\n",
      "Iteration: 280\n",
      "Iteration: 281\n",
      "Iteration: 282\n",
      "Iteration: 283\n",
      "Iteration: 284\n",
      "Iteration: 285\n",
      "Iteration: 286\n",
      "Iteration: 287\n",
      "Iteration: 288\n",
      "Iteration: 289\n",
      "Iteration: 290\n",
      "Iteration: 291\n",
      "Iteration: 292\n",
      "Iteration: 293\n",
      "Iteration: 294\n",
      "Iteration: 295\n",
      "Iteration: 296\n",
      "Iteration: 297\n",
      "Iteration: 298\n",
      "Iteration: 299\n",
      "Iteration: 300\n",
      "Iteration: 301\n",
      "Iteration: 302\n",
      "Iteration: 303\n",
      "Iteration: 304\n",
      "Iteration: 305\n",
      "Iteration: 306\n",
      "Iteration: 307\n",
      "Iteration: 308\n",
      "                                               acc_precip  mean_temp  \\\n",
      "cell_id                   datetime                                     \n",
      "(717514.445, 6177963.565) 2023-01-01 00:00:00         1.1        9.4   \n",
      "                          2023-01-01 01:00:00         3.5        9.3   \n",
      "                          2023-01-01 02:00:00         0.9        9.8   \n",
      "                          2023-01-01 03:00:00         2.3       10.9   \n",
      "                          2023-01-01 04:00:00         0.7       10.9   \n",
      "\n",
      "                                               mean_relative_hum  \\\n",
      "cell_id                   datetime                                 \n",
      "(717514.445, 6177963.565) 2023-01-01 00:00:00               98.0   \n",
      "                          2023-01-01 01:00:00               98.7   \n",
      "                          2023-01-01 02:00:00               98.8   \n",
      "                          2023-01-01 03:00:00               96.7   \n",
      "                          2023-01-01 04:00:00               96.0   \n",
      "\n",
      "                                               mean_wind_speed  mean_radiation  \n",
      "cell_id                   datetime                                              \n",
      "(717514.445, 6177963.565) 2023-01-01 00:00:00              NaN             0.0  \n",
      "                          2023-01-01 01:00:00              NaN             0.0  \n",
      "                          2023-01-01 02:00:00              NaN             0.0  \n",
      "                          2023-01-01 03:00:00              NaN             0.0  \n",
      "                          2023-01-01 04:00:00              NaN             0.0  \n"
     ]
    }
   ],
   "source": [
    "#combine all the files into one dataframe\n",
    "import os\n",
    "import pandas as pd\n",
    "folder_path = \"/Users/asherkite/Desktop/School/Courses/MBML/MBML_Project/MBML_github/MBML-traffic-accidents/Asher/Exports/Weather\"\n",
    "\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "dfs = []\n",
    "\n",
    "itnum = 0\n",
    "for file in csv_files:\n",
    "    itnum+=1\n",
    "    print(\"Iteration:\",itnum)\n",
    "    df = pd.read_csv(os.path.join(folder_path, file))\n",
    "    \n",
    "    # Parse the spatial coordinate string into a tuple, if needed\n",
    "    df['cell_id'] = df['cell_id'].apply(eval)  # Converts string like \"(x, y)\" to tuple (x, y)\n",
    "\n",
    "    # Set MultiIndex: (cell_id, datetime)\n",
    "    df.set_index(['cell_id', 'datetime'], inplace=True)\n",
    "    \n",
    "    # Convert datetime to actual datetime object\n",
    "    df.index = pd.MultiIndex.from_tuples(\n",
    "        [(cell, pd.to_datetime(time)) for cell, time in df.index],\n",
    "        names=['cell_id', 'datetime']\n",
    "    )\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all vertically — no column renaming, just stacking\n",
    "combined_df = pd.concat(dfs).sort_index()\n",
    "\n",
    "print(combined_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv (\"/Users/asherkite/Desktop/School/Courses/MBML/MBML_Project/MBML_github/FullWeather.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
